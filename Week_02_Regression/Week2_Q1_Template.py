# -*- coding: utf-8 -*-
"""Week2_Template_NR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JHbIfKN5lTgvibrMxcIPqy7rv9py4O76

# Nonlinear  Regression

Fill the blank spaces as required. 

Do not change name of any class, method name.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from matplotlib import pyplot as plt

# %matplotlib inline

class nlr:
  # Evaluates the gradient of cost function (J). Hint: You can use this to optimize w
  def grad(self,x,y,w):
    # x_vect = np.array[1,x,x**2]
    # J = np.dot(w,x_vect) - y
    # grad_J = np.multiply(J,x_vect)
    
    # mat = np.vstack([np.ones(x.size),x,np.square(x)])
    x = x.T
    rows, columns = x.shape
    J = (np.dot(w,x) - y)
    grad_J = np.dot(x,J.T)
    grad_J = np.divide(grad_J,columns)
    # print(grad_J)
    return grad_J

  # This function calculates the cost (J)
  def computeCost(self,x,y,w):
    
    # J is cost function
    # write your code to calculate J
    
    # mat = np.vstack([np.ones(x.size),x,np.square(x)])
    x = x.T
    rows, columns = x.shape
    k = (np.dot(w,x) - y)
    k = np.square(k)
    J = np.sum(k)/(2*columns)

    return J
  
  # This function optimizes the weights w_0, w_1, w_2. Batch Gradient Descent method
  def BgradientDescent(self, x, y, w, alpha, iters):
    # print(x.size)
    rows = x.size  # number of training examples
    w = w.copy() # To keep a copy of original weights
    
    J_history = []   # Use a python list to save cost in every iteration

    for i in range(iters):
      
      j = self.computeCost(x,y,w)
      grad_J = self.grad(x,y,w)
      w = np.subtract(w,np.multiply(alpha,grad_J))
      J_history.append(j)
      
      # Loop to update weights (w vector)
      # Also save cost at every step

    return w, J_history
  
  # This function optimizes the weights w_0, w_1, w_2. Stochastic Gradient Descent method
  def SgradientDescent(self, x, y, w, alpha, iters):
    m = x.size  # number of training examples
    w = w.copy() # To keep a copy of original weights
    
    J_history_s = []   # Use a python list to save cost in every iteration
    batch_size = 30
    for i in range(iters):
      r = np.random.randint(m-batch_size)
      x_arr = x[r:r+batch_size][:]
      y_arr = y[r:r+batch_size]
      
      j = self.computeCost(x_arr,y_arr,w)
      grad_J = self.grad(x_arr,y_arr,w)
      w = np.subtract(w,np.multiply(alpha,grad_J))
      J_history_s.append(j)
      
      # generate random integers (refer lab demo code)
      # create randomly a minibatch from whole data set and find weights based on that new data set.
      # Loop to update weights (w vector)
      # Also save cost at every step

    return w, J_history_s
  
  # This function implements line search Secant method
  # refer to class notes on optimization and lab demo copy.
  def ls_secant(self,x,y,w,d):
    # d is search direction d = -grad(J). Refer class and Lab notes
    epsilon = 10**(-4) # Line search tolerance
    
    alpha_old = 0   # Alpha (x_i-1)
    alpha_new = 0.01      # initial value (x_i)

    dphi_zero = np.dot(d,self.grad(x,y,w))           # dphi_zero = (d^T)(grad J(w_0) # At every alpha updation loop you will have a given initial weight vector (w_0)
    dphi_curr = dphi_zero  # required for first alpha iteration
    i = 0
    while abs(dphi_curr) > (epsilon*abs(dphi_zero)):  # tolerance or looping criteria used here
      # write loop to update alpha
      w_n = np.add(w,np.multiply(alpha_new,d))
      dphi = np.dot(d,self.grad(x,y,w_n))
      alpha_next = (dphi*alpha_old - dphi_curr*alpha_new)/(dphi-dphi_curr)
      alpha_old = alpha_new
      alpha_new = alpha_next
      dphi_curr = dphi
      # print(alpha)
    return alpha_new

  # # This function optimizes the weights w_0, w_1, w_2. Batch Gradient Descent method using variable alpha which you previously updated using ls_secant() method
  def AgradientDescent(self,x, y, w, iters):
    m = x.size  # number of training examples
    w = w.copy() # To keep a copy of original weights
    eps = 10**(-12); # tolerance for J_history

    J_history_a = [0]   # Use a python list to save cost in every iteration
    for i in range(iters):
      d = np.multiply(-1,self.grad(x,y,w))# d is search direction d = -grad(J)
      alpha = self.ls_secant(x,y,w,d) # update alpha at every iteration
      j = self.computeCost(x,y,w)
      grad_J = self.grad(x,y,w)
      w = np.subtract(w,np.multiply(alpha,grad_J))
      J_history_a.append(j)
      
      if abs(J_history_a[i+1] - J_history_a[i]) < eps:
        print('No. of iterations',i)
        break

    return w,J_history_a

