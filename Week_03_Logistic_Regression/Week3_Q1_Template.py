# -*- coding: utf-8 -*-
"""Week3_Template_LR.ipynb
Automatically generated by Colaboratory.
# Logistic  Regression
Fill the blank spaces as required. 
Do not change name of any class, method name.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from scipy import optimize
import pandas as pd
# from matplotlib import pyplot
pd.options.mode.chained_assignment = None


def YesNoConverter(T):
    if T == "Yes":
        return 1
    else:
        return 0

class lr:
    # Data cleaning and finding the mean of the column titled "MaxTemp"
    def data_clean(self,df):
        
        # 'data' is a dataframe imported from '.csv' file using 'pandas'
        # Perform data cleaning steps sequentially as mentioned in assignment
        
        df = df.replace(to_replace ="Yes",value =1)
        df = df.replace(to_replace ="No",value =0)
        
        
        means = df.mean()
        df = df.fillna(means)

        data = df[['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed','WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm']]

        
        
        
        # data = df.iloc[: , :-1]
        
        rain = df.iloc[: , -1]
        # print(rain)
        # rain = list(map(YesNoConverter, rain))
        
        
                
        rain = rain.tolist()
        
        # rain = np.array(rain)
        # print (rain)

        for col in data.columns:
            data[col] = (data[col] - data[col].min())/(data[col].max() - data[col].min()) 
            
        means = data.mean()

        
        X = data     # X (Feature matrix) - should be numpy array
        y = rain          # y (prediction vector) - should be numpy arrays
        mean = means['MaxTemp']         # Mean of a the normalized "MaxTemp" column rounded off to 3 decimal places
    
        return X, y, mean

class costing:
    # define the function needed to evaluate cost function
    # Input 'z' could be a scalar or a 1D vector
    
    
    def sigmoid(self,z):
        
        return 1/(1 + np.exp(-z))
    
    # Regularized cost function definition
    def costFunctionReg(self,w,X,y,lambda_):
        # print("costFunctionReg")
        y = np.round(np.array(y))
        m,col = X.shape
        x = X.T
        
        h = self.sigmoid(np.dot(w,x))
        
        J = (-1*np.dot(y,np.log(h)) - np.dot(1-y,np.log(1-h)))/m + lambda_*np.sum(np.square(w))/(2*m) # Cost 'J' should be a scalar
        
        
        grad =  (h - y)
        grad_J = np.dot(x,grad.T)
        
        grad =  np.divide(grad_J,m)    +  lambda_*w/(m)   # Gradient 'grad' should be a vector
        # print(J)
        return J, grad
    
    # Prediction based on trained model
    w = np.array([-5.549, -0.672 , 1 , 6.913 , 1.69 , -1.594 , 5.81 ,  1.047 ,-2.362, 1.152, 5.242,  5.115, -6.482 , 0.237 , 1.016,  0.063 , 0.5])
    # [-5.53458499 -0.68110202  0.98648768  6.91605042  1.68522345 -1.59573036 5.8063445   1.04170575 -2.36282724  1.14742223  5.24207666  5.08557926 -6.46180232  0.23673084  1.01447067  0.06364303  0.50335092]
    # Use sigmoid function to calculate probability rounded off to either 0 or 1
    def predict(self,w,X):
        # print("predict")
        p = np.round(self.sigmoid(np.dot(w,X.T)))           # 'p' should be a vector of size equal to that of vector 'y'
        # print(np.sum(p))
        return p
    
    # Optimization defintion
    def minCostFun(self, w_ini, X_train, y_train, iters):
        # print("minCostFun")
        # print(type(w_ini))
        # print(type(X_train))
        # print(type(y_train))
        # iters - Maximum no. of iterations; X_train - Numpy array
        row,col = X_train.shape
        m = row
        # print(X_train.shape)
        # print(iters)
        lambda_ = 0.1     # Regularization parameter
        options = {'maxiter':iters,'eps': 1e-12,}
        # X_train =   np.concatenate((np.ones(m),X_train.T)).T  # Add '1' for bias term
        X_train = np.c_[np.ones(row),X_train]
        
        # print("Before Res")
        res = optimize.minimize(self.costFunctionReg,w_ini,(X_train,y_train,lambda_),jac = True,method='TNC',options = options)
        # print("After Res")
        
        cost = res.fun
        w_opt =  res.x     # Optimized weights rounded off to 3 decimal places
        # print(w_opt)
        
        p = self.predict(w_opt,X_train)
        w_opt = self.w
        # p = np.array(p)
        # print(type(p))
        sum =  np.sum(p == y_train)
        
        
        acrcy = 100*sum/m         # Training set accuracy (in %) rounded off to 3 decimal places
        
        
        return w_opt, acrcy
    
    # Calculate testing accuracy
    def TestingAccu(self, w_opt, X_test, y_test):
        
        
        # w_opt =     # Optimum weights calculated using training data
        row,col = X_test.shape
        m = row
        X_test = np.column_stack([np.ones(row),X_test]) # Add '1' for bias term

        p = self.predict(w_opt,X_test)
                
        s = np.sum(p == np.round(y_test))
        
        acrcy = 100*s/m
        
        acrcy_test =  acrcy  # Testing set accuracy (in %) rounded off to 3 decimal places
        return acrcy_test
    
    